{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CkgXWUeLvffV","executionInfo":{"status":"ok","timestamp":1643190282408,"user_tz":-60,"elapsed":111071,"user":{"displayName":"GIULIA CALVO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipiBRXzM9AJKkI-C5wbQkywtQhj4pPSo8WI5hH=s64","userId":"00694631904324240896"}},"outputId":"48ce2eca-e1f2-4ada-bb18-2ebbfbe5a1ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["import urllib.request\n","import os\n","\n","def download_file(url,local_file, force=False):\n","    \"\"\"\n","    Helper function to download a file and store it locally\n","    \"\"\"\n","    if not os.path.exists(local_file) or force:\n","        print('Downloading',url,'to',local_file)\n","        with urllib.request.urlopen(url) as opener, \\\n","             open(local_file, mode='w', encoding='utf-8') as outfile:\n","                    outfile.write(opener.read().decode('utf-8'))\n","    else:\n","        print(local_file,'already downloaded')"],"metadata":{"id":"SGLUuslixCHe","executionInfo":{"status":"ok","timestamp":1643192432356,"user_tz":-60,"elapsed":245,"user":{"displayName":"GIULIA CALVO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipiBRXzM9AJKkI-C5wbQkywtQhj4pPSo8WI5hH=s64","userId":"00694631904324240896"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["train_file = 'drive/MyDrive/Text Analytics/training_set.csv'\n","test_file = 'drive/MyDrive/Text Analytics/test_set.csv'\n","delimiter = \".@\"\n"],"metadata":{"id":"6VaITowmxCeY","executionInfo":{"status":"ok","timestamp":1643192977303,"user_tz":-60,"elapsed":223,"user":{"displayName":"GIULIA CALVO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipiBRXzM9AJKkI-C5wbQkywtQhj4pPSo8WI5hH=s64","userId":"00694631904324240896"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","df=pd.read_csv('drive/MyDrive/Text Analytics/training_set.csv')"],"metadata":{"id":"kbUR7rKk6Y_T","executionInfo":{"status":"ok","timestamp":1643192978228,"user_tz":-60,"elapsed":3,"user":{"displayName":"GIULIA CALVO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipiBRXzM9AJKkI-C5wbQkywtQhj4pPSo8WI5hH=s64","userId":"00694631904324240896"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import csv\n","x_train = list()\n","y_train = list()\n","with open(train_file, encoding='utf-8', newline='') as infile:\n","    reader = csv.reader(infile, delimiter=delimiter)\n","    for row in reader:\n","      #if row:\n","        x_train.append(row[0])\n","        y_train.append(row[1])\n","\n","x_test = list()\n","y_test = list()\n","with open(test_file, encoding='utf-8', newline='') as infile:\n","    reader = csv.reader(infile, delimiter=delimiter)\n","    for row in reader:\n","      #if row:\n","        x_test.append(row[0])\n","        y_test.append(row[1])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"3bTz3s1OxCg1","executionInfo":{"status":"error","timestamp":1643192979132,"user_tz":-60,"elapsed":9,"user":{"displayName":"GIULIA CALVO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipiBRXzM9AJKkI-C5wbQkywtQhj4pPSo8WI5hH=s64","userId":"00694631904324240896"}},"outputId":"e12f54c8-0c11-4afe-c2b3-95658f660194"},"execution_count":17,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-4cd465add71a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;31m#if row:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: \"delimiter\" must be a 1-character string"]}]},{"cell_type":"code","source":["set(y_train)"],"metadata":{"id":"hJpYSwIFxCnj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_idx = 10\n","x_train[sample_idx]"],"metadata":{"id":"0Oa4whdkxCp_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train[sample_idx]"],"metadata":{"id":"goI-ulwVxCsb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Feature extraction*"],"metadata":{"id":"wjtVP4T2yx4a"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","from nltk.corpus import stopwords, wordnet\n","from nltk import word_tokenize, pos_tag\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","stopword_list = stopwords.words('english')\n","\n","from collections import defaultdict\n","tag_map = defaultdict(lambda : wordnet.NOUN)\n","tag_map['J'] = wordnet.ADJ\n","tag_map['V'] = wordnet.VERB\n","tag_map['R'] = wordnet.ADV\n","lemmatizer = WordNetLemmatizer()\n","\n","doc_counter = 0\n","def reset_counter():\n","  global doc_counter\n","  doc_counter = 0\n","\n","def increase_counter():\n","  global doc_counter\n","  doc_counter += 1\n","  if doc_counter % 100 == 0:\n","    print(doc_counter)\n","\n","def nltk_ngram_tokenizer(text):\n","    increase_counter()\n","\n","    # tokens, skipping stopwords\n","    tokens = [token for token in word_tokenize(text) if token not in stopword_list]\n","\n","    # we use a simple nltk function to create ngrams\n","    bigrams = ['BI_'+w1+'_'+w2 for w1,w2 in nltk.ngrams(tokens,2)]\n","    trigrams = ['TRI_'+p1+'_'+p2+'_'+p3 for p1,p2,p3 in nltk.ngrams(tokens,3)]\n","\n","    all_tokens = list()\n","    all_tokens.extend(tokens)\n","    all_tokens.extend(bigrams)\n","    all_tokens.extend(trigrams)\n","    return all_tokens\n","\n","def nltk_nlp_tokenizer(text):\n","    increase_counter()\n","\n","    # tokens, skipping stopwords\n","    tokens = [token for token in word_tokenize(text) if token not in stopword_list]\n","\n","    # lemmatized tokens\n","    lemmas = list()\n","    for token, tag in pos_tag(tokens):\n","  \t    lemmas.append('LEMMA_'+lemmatizer.lemmatize(token, tag_map[tag[0]]))\n","\n","    # we use a simple nltk function to create ngrams\n","    lemma_bigrams = ['BI_'+p1+'_'+p2 for p1,p2 in nltk.ngrams(lemmas,2)]\n","    lemma_trigrams = ['TRI_'+p1+'_'+p2+'_'+p3 for p1,p2,p3 in nltk.ngrams(lemmas,3)]\n","\n","    all_tokens = list()\n","    all_tokens.extend(lemmas)\n","    all_tokens.extend(lemma_bigrams)\n","    all_tokens.extend(lemma_trigrams)\n","    return all_tokens\n","\n","import spacy\n","import re\n","nlp = spacy.load('en')\n","\n","def spacy_nlp_tokenizer(text):\n","    increase_counter()\n","\n","    # substituting all space characters with a single space\n","    text = re.sub('\\s+', ' ', text)\n","\n","    # we use spacy for main nlp tasks\n","    doc = nlp(text)\n","    # lemmatized tokens, skipping stopwords\n","    lemmas = ['LEMMA_'+token.lemma_ for token in doc if not token.is_stop]\n","    # entity_types\n","    entity_types = ['NER_'+token.ent_type_ for token in doc if token.ent_type_]\n","\n","    # in case an entity linker is available, we can use it do put actual entities as\n","    # features, e.g. Queen Elizabeth, Elizabeth II, Her Majesty -> KB2912\n","    # see https://spacy.io/usage/training#entity-linker\n","    # entities = ['ENT_'+token.ent_kb_id_ for token in doc if token.ent_kb_id_]\n","\n","    # we use a simple nltk function to create ngrams\n","    lemma_bigrams = ['BI_'+p1+'_'+p2 for p1,p2 in nltk.ngrams(lemmas,2)]\n","    lemma_trigrams = ['TRI_'+p1+'_'+p2+'_'+p3 for p1,p2,p3 in nltk.ngrams(lemmas,3)]\n","\n","    all_tokens = list()\n","    all_tokens.extend(lemmas)\n","    all_tokens.extend(lemma_bigrams)\n","    all_tokens.extend(lemma_trigrams)\n","    all_tokens.extend(entity_types)\n","    return all_tokens"],"metadata":{"id":"qxVFzt8BxCu_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenization"],"metadata":{"id":"vmIPiXb1zJnr"}},{"cell_type":"code","source":["import numpy as np\n","\n","# numpy implements many useful and powerful vector manipulation tools\n","# here I'm using it to quickly create a True,False vector corresponding\n","# to the original values being equal to our label of interest or not\n","# i.e., binary labels\n","\n","y_train_bin = np.asarray(y_train)==y_train[sample_idx]\n","y_test_bin = np.asarray(y_test)==y_train[sample_idx]\n","y_train_bin,y_test_bin"],"metadata":{"id":"ugGYreSPxCzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n","from sklearn.feature_selection import SelectKBest, chi2\n","from sklearn.pipeline import Pipeline\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import LinearSVC"],"metadata":{"id":"7YOdpNipzV8H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vect = CountVectorizer(analyzer=nltk_ngram_tokenizer, min_df=5)  # tokenization and frequency count\n","# vect = CountVectorizer(analyzer=nltk_n_gram_tokenizer)  \n","# vect = CountVectorizer(analyzer=nltk_nlp_tokenizer, min_df=5)  \n","vect = CountVectorizer(analyzer=spacy_nlp_tokenizer, min_df=5)  \n","\n","reset_counter()\n","\n","X_train_tok = vect.fit_transform(x_train)\n","\n","reset_counter()\n","\n","X_test_tok = vect.transform(x_test)"],"metadata":{"id":"r3Hs6o6_zV4q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#salva i file tikenization\n","import pickle\n","\n","with open('news_x_train_tok.pkl','wb') as outfile:\n","  pickle.dump(X_train_tok,outfile)\n","with open('news_x_test_tok.pkl','wb') as outfile:\n","  pickle.dump(X_test_tok,outfile)"],"metadata":{"id":"AO6BF3fszV2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"id":"Vs_73KjLzVzw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download('news_x_train_tok.pkl')\n","files.download('news_x_test_tok.pkl')"],"metadata":{"id":"eNN4cJy7zVqq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["files.upload()"],"metadata":{"id":"P4KuO3D3zyGM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(vect.vocabulary_)"],"metadata":{"id":"L0Pgt7NX27jr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vect.vocabulary_"],"metadata":{"id":"ZwtN1ZnQ27hY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_tok[0,:]"],"metadata":{"id":"F6bUzigK27fE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train_tok[0,:])"],"metadata":{"id":"R6xh1g-327cs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vect.inverse_transform(X_train_tok[0,:])"],"metadata":{"id":"l4I6_WrS27af"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for feat,freq in zip(vect.inverse_transform(X_train_tok[0,:])[0],X_train_tok[0,:].data):\n","  print(feat,freq)"],"metadata":{"id":"JD16Hj2727YX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Classification*"],"metadata":{"id":"9Fnz_8oi3XFT"}},{"cell_type":"code","source":["pipeline = Pipeline([\n","    ('sel', SelectKBest(chi2, k=5000)),  # feature selection\n","    ('tfidf', TfidfTransformer()),  # weighting\n","    ('learner', LinearSVC())  # learning algorithm\n","])\n","\n","classifier = pipeline.fit(X_train_tok,y_train)\n","predictions = classifier.predict(X_test_tok)\n","correct = 0\n","for prediction,true_label in zip(predictions, y_test):\n","    if prediction==true_label:\n","        correct += 1\n","print(correct/len(predictions))"],"metadata":{"id":"cOYNKmt63OuZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, classification_report\n","print('Classification report:')\n","print(classification_report(y_test, predictions))\n","print('Confusion matrix:')\n","cm = confusion_matrix(y_test, predictions)\n","print(cm)"],"metadata":{"id":"TdscnAwb3Oqh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.multiclass import OneVsOneClassifier\n","\n","pipeline = Pipeline([\n","    ('sel', SelectKBest(chi2, k=5000)),  # feature selection\n","    ('tfidf', TfidfTransformer()),  # weighting\n","    ('learner', OneVsOneClassifier(LinearSVC()))  # learning algorithm\n","])\n","\n","classifier = pipeline.fit(X_train_tok,y_train)\n","predictions = classifier.predict(X_test_tok)"],"metadata":{"id":"JIYX1JUL3OoN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, classification_report\n","print('Classification report:')\n","print(classification_report(y_test, predictions))\n","print('Confusion matrix:')\n","cm = confusion_matrix(y_test, predictions)\n","print(cm)"],"metadata":{"id":"q--XRPPr3xOP"},"execution_count":null,"outputs":[]}]}